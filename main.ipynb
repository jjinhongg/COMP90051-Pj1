{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daabaeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from STGCN.ipynb\n",
      "importing Jupyter notebook from skeleton.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from STGCN import Model\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import inspect\n",
    "import shutil\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2268ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Spatial Temporal Graph Convolutional Neural Network for Skeleton-Based Action Recognition')\n",
    "    parser.add_argument(\n",
    "        '--base-lr', type=float, default=1e-1, help='initial learning rate')\n",
    "    parser.add_argument(\n",
    "        '--num-classes', type=int, default=49, help='number of classes in dataset')\n",
    "    parser.add_argument(\n",
    "        '--batch-size', type=int, default=64, help='training batch size')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs', type=int, default=50, help='total epochs to train')\n",
    "    parser.add_argument(\n",
    "        '--save-freq', type=int, default=10, help='periodicity of saving model weights')\n",
    "    parser.add_argument(\n",
    "        '--checkpoint-path',\n",
    "        default=\"checkpoints\",\n",
    "        help='folder to store model weights')\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        default=\"logs\",\n",
    "        help='folder to store model-definition/training-logs/hyperparameters')\n",
    "    parser.add_argument(\n",
    "        '--train-data-path',\n",
    "        default=\"tfrecord\",\n",
    "        help='path to folder with training dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--test-data-path',\n",
    "        default=\"tfrecord\",\n",
    "        help='path to folder with testing dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--steps',\n",
    "        type=int,\n",
    "        default=[10, 40],\n",
    "        nargs='+',\n",
    "        help='the epoch where optimizer reduce the learning rate, eg: 10 50')\n",
    "    parser.add_argument(\n",
    "        '--gpus',\n",
    "        default= None,\n",
    "        nargs='+',\n",
    "        help='list of gpus to use for training, eg: \"/gpu:0\" \"/gpu:1\"')\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a6ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, num_classes=49, batch_size=32, drop_remainder=False,\n",
    "                shuffle=False, shuffle_size=1000):\n",
    "    # dictionary describing the features.\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label'     : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    # parse each proto and, the features within\n",
    "    def _parse_feature_function(example_proto):\n",
    "        features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        data =  tf.io.parse_tensor(features['features'], tf.float32)\n",
    "        label = tf.one_hot(features['label'], num_classes)\n",
    "        data = tf.reshape(data, (3, 16, 20, 1))\n",
    "        return data, label\n",
    "\n",
    "    records = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\"tfrecord\")]\n",
    "    dataset = tf.data.TFRecordDataset(records, num_parallel_reads=len(records))\n",
    "    dataset = dataset.map(_parse_feature_function)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862c0830",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    def step_fn(features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(features, training=True)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                              labels=labels)\n",
    "            loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        train_acc(labels, logits)\n",
    "        train_acc_top_5(labels, logits)\n",
    "        cross_entropy_loss(loss)\n",
    "    strategy.run(step_fn, args=(features, labels,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44717555",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(features):\n",
    "    logits = model(features, training=False)\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9450d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Num GPUs Available:  0\n",
      "***************\n",
      "(64, 3, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 384, 16, 20)\n",
      "64 384 16 20\n",
      "after:  (64, 3, 128, 16, 20)\n",
      "***************\n",
      "(64, 128, 8, 20)\n",
      "(64, 384, 8, 20)\n",
      "64 384 8 20\n",
      "after:  (64, 3, 128, 8, 20)\n",
      "***************\n",
      "(64, 128, 8, 20)\n",
      "(64, 384, 8, 20)\n",
      "64 384 8 20\n",
      "after:  (64, 3, 128, 8, 20)\n",
      "***************\n",
      "(64, 128, 8, 20)\n",
      "(64, 768, 8, 20)\n",
      "64 768 8 20\n",
      "after:  (64, 3, 256, 8, 20)\n",
      "***************\n",
      "(64, 256, 4, 20)\n",
      "(64, 768, 4, 20)\n",
      "64 768 4 20\n",
      "after:  (64, 3, 256, 4, 20)\n",
      "***************\n",
      "(64, 256, 4, 20)\n",
      "(64, 768, 4, 20)\n",
      "64 768 4 20\n",
      "after:  (64, 3, 256, 4, 20)\n",
      "***************\n",
      "(64, 3, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 192, 16, 20)\n",
      "64 192 16 20\n",
      "after:  (64, 3, 64, 16, 20)\n",
      "***************\n",
      "(64, 64, 16, 20)\n",
      "(64, 384, 16, 20)\n",
      "64 384 16 20\n",
      "after:  (64, 3, 128, 16, 20)\n",
      "***************\n",
      "(64, 128, 8, 20)\n",
      "(64, 384, 8, 20)\n",
      "64 384 8 20\n",
      "after:  (64, 3, 128, 8, 20)\n",
      "***************\n",
      "(64, 128, 8, 20)\n",
      "(64, 384, 8, 20)\n",
      "64 384 8 20\n",
      "after:  (64, 3, 128, 8, 20)\n",
      "***************\n",
      "(64, 128, 8, 20)\n",
      "(64, 768, 8, 20)\n",
      "64 768 8 20\n",
      "after:  (64, 3, 256, 8, 20)\n",
      "***************\n",
      "(64, 256, 4, 20)\n",
      "(64, 768, 4, 20)\n",
      "64 768 4 20\n",
      "after:  (64, 3, 256, 4, 20)\n",
      "***************\n",
      "(64, 256, 4, 20)\n",
      "(64, 768, 4, 20)\n",
      "64 768 4 20\n",
      "after:  (64, 3, 256, 4, 20)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Conv2DCustomBackpropInputOp only supports NHWC.\n\t [[node gradient_tape/model/conv2d_22/Conv2D/Conv2DBackpropInput (defined at /var/folders/kw/mbmklccd4bnb7gnrvg3gp_sm0000gn/T/tmpp2wfwcpd.py:16) ]] [Op:__inference_train_step_5495]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/model/conv2d_22/Conv2D/Conv2DBackpropInput:\n model/conv2d_22/Conv2D/ReadVariableOp (defined at <string>:56)\n\nFunction call stack:\ntrain_step\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f2a9d61c5fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# variables, which is required for the first call of @tf.function function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training_trace\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Conv2DCustomBackpropInputOp only supports NHWC.\n\t [[node gradient_tape/model/conv2d_22/Conv2D/Conv2DBackpropInput (defined at /var/folders/kw/mbmklccd4bnb7gnrvg3gp_sm0000gn/T/tmpp2wfwcpd.py:16) ]] [Op:__inference_train_step_5495]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/model/conv2d_22/Conv2D/Conv2DBackpropInput:\n model/conv2d_22/Conv2D/ReadVariableOp (defined at <string>:56)\n\nFunction call stack:\ntrain_step\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = get_parser()\n",
    "    arg, unknown = parser.parse_known_args()\n",
    "\n",
    "    base_lr         = arg.base_lr\n",
    "    num_classes     = arg.num_classes\n",
    "    epochs          = arg.num_epochs\n",
    "    checkpoint_path = arg.checkpoint_path\n",
    "    log_dir         = arg.log_dir\n",
    "    train_data_path = arg.train_data_path\n",
    "    test_data_path  = arg.test_data_path\n",
    "    save_freq       = arg.save_freq\n",
    "    steps           = arg.steps\n",
    "    batch_size      = arg.batch_size\n",
    "    gpus            = arg.gpus\n",
    "    strategy        = tf.distribute.MirroredStrategy(arg.gpus)\n",
    "    global_batch_size = arg.batch_size*strategy.num_replicas_in_sync\n",
    "    arg.gpus        = strategy.num_replicas_in_sync\n",
    "\n",
    "    #copy hyperparameters and model definition to log folder\n",
    "    #save_arg(arg)\n",
    "    shutil.copy2(inspect.getfile(Model), arg.log_dir)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    '''\n",
    "    Get tf.dataset objects for training and testing data\n",
    "    Data shape: features - batch_size, 3, 300, 25, 2\n",
    "                labels   - batch_size, num_classes\n",
    "    '''\n",
    "    train_data = get_dataset(train_data_path,\n",
    "                             num_classes=num_classes,\n",
    "                             batch_size=global_batch_size,\n",
    "                             drop_remainder=True,\n",
    "                             shuffle=True)\n",
    "    train_data = strategy.experimental_distribute_dataset(train_data)\n",
    "\n",
    "    test_data = get_dataset(test_data_path,\n",
    "                            num_classes=num_classes,\n",
    "                            batch_size=batch_size,\n",
    "                            drop_remainder=False,\n",
    "                            shuffle=False)\n",
    "\n",
    "    boundaries = [(step*40000)//batch_size for step in steps]\n",
    "    values = [base_lr]*(len(steps)+1)\n",
    "    for i in range(1, len(steps)+1):\n",
    "        values[i] *= 0.1**i\n",
    "    learning_rate  = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model        = Model(num_classes=num_classes)\n",
    "        optimizer    = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                               momentum=0.9,\n",
    "                                               nesterov=True)\n",
    "        ckpt         = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "        ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                                  checkpoint_path,\n",
    "                                                  max_to_keep=5)\n",
    "\n",
    "        # keras metrics to hold accuracies and loss\n",
    "        cross_entropy_loss   = tf.keras.metrics.Mean(name='cross_entropy_loss')\n",
    "        train_acc            = tf.keras.metrics.CategoricalAccuracy(name='train_acc')\n",
    "        train_acc_top_5      = tf.keras.metrics.TopKCategoricalAccuracy(name='train_acc_top_5')\n",
    "\n",
    "    epoch_test_acc       = tf.keras.metrics.CategoricalAccuracy(name='epoch_test_acc')\n",
    "    epoch_test_acc_top_5 = tf.keras.metrics.TopKCategoricalAccuracy(name='epoch_test_acc_top_5')\n",
    "    test_acc_top_5       = tf.keras.metrics.TopKCategoricalAccuracy(name='test_acc_top_5')\n",
    "    test_acc             = tf.keras.metrics.CategoricalAccuracy(name='test_acc')\n",
    "    summary_writer       = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    # Get 1 batch from train dataset to get graph trace of train and test functions\n",
    "    \n",
    "    for data in test_data:\n",
    "        features, labels = data\n",
    "        break\n",
    "\n",
    "    # add graph of train and test functions to tensorboard graphs\n",
    "    # Note:\n",
    "    # graph training is True on purpose, allows tensorflow to get all the\n",
    "    # variables, which is required for the first call of @tf.function function\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    train_step(features, labels)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"training_trace\",step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    test_step(features)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"testing_trace\", step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    # start training\n",
    "    train_iter = 0\n",
    "    test_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch+1))\n",
    "        print(\"Training: \")\n",
    "        with strategy.scope():\n",
    "            for features, labels in tqdm(train_data):\n",
    "                train_step(features, labels)\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\"cross_entropy_loss\",\n",
    "                                      cross_entropy_loss.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc\",\n",
    "                                      train_acc.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc_top_5\",\n",
    "                                      train_acc_top_5.result(),\n",
    "                                      step=train_iter)\n",
    "                cross_entropy_loss.reset_states()\n",
    "                train_acc.reset_states()\n",
    "                train_acc_top_5.reset_states()\n",
    "                train_iter += 1\n",
    "\n",
    "        print(\"Testing: \")\n",
    "        for features, labels in tqdm(test_data):\n",
    "            y_pred = test_step(features)\n",
    "            test_acc(labels, y_pred)\n",
    "            epoch_test_acc(labels, y_pred)\n",
    "            test_acc_top_5(labels, y_pred)\n",
    "            epoch_test_acc_top_5(labels, y_pred)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"test_acc\",\n",
    "                                  test_acc.result(),\n",
    "                                  step=test_iter)\n",
    "                tf.summary.scalar(\"test_acc_top_5\",\n",
    "                                  test_acc_top_5.result(),\n",
    "                                  step=test_iter)\n",
    "            test_acc.reset_states()\n",
    "            test_acc_top_5.reset_states()\n",
    "            test_iter += 1\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"epoch_test_acc\",\n",
    "                              epoch_test_acc.result(),\n",
    "                              step=epoch)\n",
    "            tf.summary.scalar(\"epoch_test_acc_top_5\",\n",
    "                              epoch_test_acc_top_5.result(),\n",
    "                              step=epoch)\n",
    "        epoch_test_acc.reset_states()\n",
    "        epoch_test_acc_top_5.reset_states()\n",
    "\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                ckpt_save_path))\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print('Saving final checkpoint for epoch {} at {}'.format(epochs,\n",
    "                                                              ckpt_save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
