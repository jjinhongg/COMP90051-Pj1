{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04255320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from STGCN.ipynb\n",
      "importing Jupyter notebook from skeleton.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from STGCN import Model\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import inspect\n",
    "import shutil\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6a60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Spatial Temporal Graph Convolutional Neural Network for Skeleton-Based Action Recognition')\n",
    "    parser.add_argument(\n",
    "        '--base-lr', type=float, default=1e-1, help='initial learning rate')\n",
    "    parser.add_argument(\n",
    "        '--num-classes', type=int, default=49, help='number of classes in dataset')\n",
    "    parser.add_argument(\n",
    "        '--batch-size', type=int, default=64, help='training batch size')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs', type=int, default=50, help='total epochs to train')\n",
    "    parser.add_argument(\n",
    "        '--save-freq', type=int, default=10, help='periodicity of saving model weights')\n",
    "    parser.add_argument(\n",
    "        '--checkpoint-path',\n",
    "        default=\"checkpoints\",\n",
    "        help='folder to store model weights')\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        default=\"logs\",\n",
    "        help='folder to store model-definition/training-logs/hyperparameters')\n",
    "    parser.add_argument(\n",
    "        '--train-data-path',\n",
    "        default=\"tfrecord\",\n",
    "        help='path to folder with training dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--test-data-path',\n",
    "        default=\"tfrecord\",\n",
    "        help='path to folder with testing dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--steps',\n",
    "        type=int,\n",
    "        default=[10, 40],\n",
    "        nargs='+',\n",
    "        help='the epoch where optimizer reduce the learning rate, eg: 10 50')\n",
    "    parser.add_argument(\n",
    "        '--gpus',\n",
    "        default= None,\n",
    "        nargs='+',\n",
    "        help='list of gpus to use for training, eg: \"/gpu:0\" \"/gpu:1\"')\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b01d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, num_classes=49, batch_size=32, drop_remainder=False,\n",
    "                shuffle=False, shuffle_size=1000):\n",
    "    # dictionary describing the features.\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label'     : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    # parse each proto and, the features within\n",
    "    def _parse_feature_function(example_proto):\n",
    "        features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        data =  tf.io.parse_tensor(features['features'], tf.float32)\n",
    "        label = tf.one_hot(features['label'], num_classes)\n",
    "        data = tf.reshape(data, (3, 16, 20, 1))\n",
    "        return data, label\n",
    "\n",
    "    records = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\"tfrecord\")]\n",
    "    dataset = tf.data.TFRecordDataset(records, num_parallel_reads=len(records))\n",
    "    dataset = dataset.map(_parse_feature_function)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d930318",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    def step_fn(features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(features, training=True)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                              labels=labels)\n",
    "            loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        train_acc(labels, logits)\n",
    "        train_acc_top_5(labels, logits)\n",
    "        cross_entropy_loss(loss)\n",
    "    strategy.run(step_fn, args=(features, labels,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64451a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(features):\n",
    "    logits = model(features, training=False)\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b5f11c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown attribute: 'g' in 'g'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f2a9d61c5fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mbatch_size\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mgpus\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mstrategy\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mglobal_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_replicas_in_sync\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_replicas_in_sync\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, cross_device_ops)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_device_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     extended = MirroredExtended(\n\u001b[0m\u001b[1;32m    282\u001b[0m         self, devices=devices, cross_device_ops=cross_device_ops)\n\u001b[1;32m    283\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, container_strategy, devices, cross_device_ops)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMirroredExtended\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mdevices\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_device_list_single_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         raise RuntimeError(\"In-graph multi-worker training with \"\n\u001b[1;32m    317\u001b[0m                            \"`MirroredStrategy` is not supported in eager mode.\")\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py\u001b[0m in \u001b[0;36m_is_device_list_single_worker\u001b[0;34m(devices)\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogicalDevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mspecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplica\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0mall_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/device_spec.py\u001b[0m in \u001b[0;36mfrom_string\u001b[0;34m(cls, spec)\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mDeviceSpec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string_to_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mparse_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/device_spec.py\u001b[0m in \u001b[0;36m_string_to_components\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mdevice_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mly\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=g-explicit-bool-comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown attribute: '%s' in '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown attribute: 'g' in 'g'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = get_parser()\n",
    "    arg, unknown = parser.parse_known_args()\n",
    "\n",
    "    base_lr         = arg.base_lr\n",
    "    num_classes     = arg.num_classes\n",
    "    epochs          = arg.num_epochs\n",
    "    checkpoint_path = arg.checkpoint_path\n",
    "    log_dir         = arg.log_dir\n",
    "    train_data_path = arg.train_data_path\n",
    "    test_data_path  = arg.test_data_path\n",
    "    save_freq       = arg.save_freq\n",
    "    steps           = arg.steps\n",
    "    batch_size      = arg.batch_size\n",
    "    gpus            = arg.gpus\n",
    "    strategy        = tf.distribute.MirroredStrategy(arg.gpus)\n",
    "    global_batch_size = arg.batch_size*strategy.num_replicas_in_sync\n",
    "    arg.gpus        = strategy.num_replicas_in_sync\n",
    "\n",
    "    #copy hyperparameters and model definition to log folder\n",
    "    #save_arg(arg)\n",
    "    shutil.copy2(inspect.getfile(Model), arg.log_dir)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    '''\n",
    "    Get tf.dataset objects for training and testing data\n",
    "    Data shape: features - batch_size, 3, 300, 25, 2\n",
    "                labels   - batch_size, num_classes\n",
    "    '''\n",
    "    train_data = get_dataset(train_data_path,\n",
    "                             num_classes=num_classes,\n",
    "                             batch_size=global_batch_size,\n",
    "                             drop_remainder=True,\n",
    "                             shuffle=True)\n",
    "    train_data = strategy.experimental_distribute_dataset(train_data)\n",
    "\n",
    "    test_data = get_dataset(test_data_path,\n",
    "                            num_classes=num_classes,\n",
    "                            batch_size=batch_size,\n",
    "                            drop_remainder=False,\n",
    "                            shuffle=False)\n",
    "\n",
    "    boundaries = [(step*40000)//batch_size for step in steps]\n",
    "    values = [base_lr]*(len(steps)+1)\n",
    "    for i in range(1, len(steps)+1):\n",
    "        values[i] *= 0.1**i\n",
    "    learning_rate  = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model        = Model(num_classes=num_classes)\n",
    "        optimizer    = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                               momentum=0.9,\n",
    "                                               nesterov=True)\n",
    "        ckpt         = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "        ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                                  checkpoint_path,\n",
    "                                                  max_to_keep=5)\n",
    "\n",
    "        # keras metrics to hold accuracies and loss\n",
    "        cross_entropy_loss   = tf.keras.metrics.Mean(name='cross_entropy_loss')\n",
    "        train_acc            = tf.keras.metrics.CategoricalAccuracy(name='train_acc')\n",
    "        train_acc_top_5      = tf.keras.metrics.TopKCategoricalAccuracy(name='train_acc_top_5')\n",
    "\n",
    "    epoch_test_acc       = tf.keras.metrics.CategoricalAccuracy(name='epoch_test_acc')\n",
    "    epoch_test_acc_top_5 = tf.keras.metrics.TopKCategoricalAccuracy(name='epoch_test_acc_top_5')\n",
    "    test_acc_top_5       = tf.keras.metrics.TopKCategoricalAccuracy(name='test_acc_top_5')\n",
    "    test_acc             = tf.keras.metrics.CategoricalAccuracy(name='test_acc')\n",
    "    summary_writer       = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    # Get 1 batch from train dataset to get graph trace of train and test functions\n",
    "    \n",
    "    for data in test_data:\n",
    "        features, labels = data\n",
    "        break\n",
    "\n",
    "    # add graph of train and test functions to tensorboard graphs\n",
    "    # Note:\n",
    "    # graph training is True on purpose, allows tensorflow to get all the\n",
    "    # variables, which is required for the first call of @tf.function function\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    train_step(features, labels)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"training_trace\",step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    test_step(features)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"testing_trace\", step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    # start training\n",
    "    train_iter = 0\n",
    "    test_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch+1))\n",
    "        print(\"Training: \")\n",
    "        with strategy.scope():\n",
    "            for features, labels in tqdm(train_data):\n",
    "                train_step(features, labels)\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\"cross_entropy_loss\",\n",
    "                                      cross_entropy_loss.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc\",\n",
    "                                      train_acc.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc_top_5\",\n",
    "                                      train_acc_top_5.result(),\n",
    "                                      step=train_iter)\n",
    "                cross_entropy_loss.reset_states()\n",
    "                train_acc.reset_states()\n",
    "                train_acc_top_5.reset_states()\n",
    "                train_iter += 1\n",
    "\n",
    "        print(\"Testing: \")\n",
    "        for features, labels in tqdm(test_data):\n",
    "            y_pred = test_step(features)\n",
    "            test_acc(labels, y_pred)\n",
    "            epoch_test_acc(labels, y_pred)\n",
    "            test_acc_top_5(labels, y_pred)\n",
    "            epoch_test_acc_top_5(labels, y_pred)\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"test_acc\",\n",
    "                                  test_acc.result(),\n",
    "                                  step=test_iter)\n",
    "                tf.summary.scalar(\"test_acc_top_5\",\n",
    "                                  test_acc_top_5.result(),\n",
    "                                  step=test_iter)\n",
    "            test_acc.reset_states()\n",
    "            test_acc_top_5.reset_states()\n",
    "            test_iter += 1\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"epoch_test_acc\",\n",
    "                              epoch_test_acc.result(),\n",
    "                              step=epoch)\n",
    "            tf.summary.scalar(\"epoch_test_acc_top_5\",\n",
    "                              epoch_test_acc_top_5.result(),\n",
    "                              step=epoch)\n",
    "        epoch_test_acc.reset_states()\n",
    "        epoch_test_acc_top_5.reset_states()\n",
    "\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                ckpt_save_path))\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print('Saving final checkpoint for epoch {} at {}'.format(epochs,\n",
    "                                                              ckpt_save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
