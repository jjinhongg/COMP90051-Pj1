{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import import_ipynb\n",
    "from STGCN import Model\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import inspect\n",
    "import shutil\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Spatial Temporal Graph Convolutional Neural Network for Skeleton-Based Action Recognition')\n",
    "    parser.add_argument(\n",
    "        '--base-lr', type=float, default=1e-1, help='initial learning rate')\n",
    "    parser.add_argument(\n",
    "        '--num-classes', type=int, default=49, help='number of classes in dataset')\n",
    "    parser.add_argument(\n",
    "        '--batch-size', type=int, default=64, help='training batch size')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs', type=int, default=25, help='total epochs to train')\n",
    "    parser.add_argument(\n",
    "        '--save-freq', type=int, default=10, help='periodicity of saving model weights')\n",
    "    parser.add_argument(\n",
    "        '--checkpoint-path',\n",
    "        default=\"checkpoints\",\n",
    "        help='folder to store model weights')\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        default=\"logs\",\n",
    "        help='folder to store model-definition/training-logs/hyperparameters')\n",
    "    parser.add_argument(\n",
    "        '--train-data-path',\n",
    "        default=\"tfrecord_train\",\n",
    "        help='path to folder with training dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--test-data-path',\n",
    "        default=\"tfrecord_valid\",\n",
    "        help='path to folder with testing dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--steps',\n",
    "        type=int,\n",
    "        default=[10, 20],\n",
    "        nargs='+',\n",
    "        help='the epoch where optimizer reduce the learning rate, eg: 10 50')\n",
    "    parser.add_argument(\n",
    "        '--gpus',\n",
    "        default= None,\n",
    "        nargs='+',\n",
    "        help='list of gpus to use for training, eg: \"/gpu:0\" \"/gpu:1\"')\n",
    "\n",
    "    return parser\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def get_dataset(directory, num_classes=49, batch_size=32, drop_remainder=False,\n",
    "                shuffle=False, shuffle_size=1000):\n",
    "    # dictionary describing the features.\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label'     : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    # parse each proto and, the features within\n",
    "    def _parse_feature_function(example_proto):\n",
    "        features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        data =  tf.io.parse_tensor(features['features'], tf.float32)\n",
    "        index = tf.one_hot(features['label'], 2959)\n",
    "        label = tf.one_hot(features['label'], num_classes)\n",
    "        data = tf.reshape(data, (3, 16, 20, 1))\n",
    "        return data, label, index\n",
    "\n",
    "    records = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\"tfrecord\")]\n",
    "    dataset = tf.data.TFRecordDataset(records, num_parallel_reads=len(records))\n",
    "    dataset = dataset.map(_parse_feature_function)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    \n",
    "    def class_weight(labels_dict,mu=0.15):\n",
    "        total = sum(labels_dict.values())\n",
    "        keys = labels_dict.keys()\n",
    "        weight = dict()\n",
    "        \n",
    "        for i in keys:\n",
    "                score = np.log(mu*total/float(labels_dict[i]))\n",
    "                weight[i] = score if score > 1 else 1\n",
    "                \n",
    "        return weight\n",
    "    \n",
    "    # random labels_dict\n",
    "    labels_dict = dict(Counter(labels))\n",
    "    weights = class_weight(labels_dict)\n",
    "    weights_list = list(weights.values())\n",
    "    class_weights = tf.constant([weights_list])\n",
    "    # deduce weights for batch samples based on their true label\n",
    "    \n",
    "    def step_fn(features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(features, training=True)\n",
    "            onehot_labels = tf.one_hot(labels)\n",
    "            weights = tf.reduce_sum(class_weights * onehot_labels, axis=1)\n",
    "            \n",
    "            # cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "            #                                                   labels=labels)\n",
    "            # compute your (unweighted) softmax cross entropy loss\n",
    "            unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(onehot_labels, logits)\n",
    "            \n",
    "            # apply the weights, relying on broadcasting of the multiplication\n",
    "            weighted_losses = unweighted_losses * weights\n",
    "            \n",
    "            # reduce the result to get your final loss\n",
    "            loss = tf.reduce_mean(weighted_losses) * (1.0 / global_batch_size)\n",
    "            \n",
    "            # loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        train_acc(labels, logits)\n",
    "        train_acc_top_5(labels, logits)\n",
    "        cross_entropy_loss(loss)\n",
    "    strategy.run(step_fn, args=(features, labels,))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "@tf.function\n",
    "def test_step(features):\n",
    "    logits = model(features, training=False)\n",
    "    return tf.nn.softmax(logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = get_parser()\n",
    "    arg, unknown = parser.parse_known_args()\n",
    "\n",
    "    base_lr         = arg.base_lr\n",
    "    num_classes     = arg.num_classes\n",
    "    epochs          = arg.num_epochs\n",
    "    checkpoint_path = arg.checkpoint_path\n",
    "    log_dir         = arg.log_dir\n",
    "    train_data_path = arg.train_data_path\n",
    "    test_data_path  = arg.test_data_path\n",
    "    save_freq       = arg.save_freq\n",
    "    steps           = arg.steps\n",
    "    batch_size      = arg.batch_size\n",
    "    gpus            = arg.gpus\n",
    "    strategy        = tf.distribute.MirroredStrategy(arg.gpus)\n",
    "    global_batch_size = arg.batch_size*strategy.num_replicas_in_sync\n",
    "    arg.gpus        = strategy.num_replicas_in_sync\n",
    "\n",
    "    #copy hyperparameters and model definition to log folder\n",
    "    #save_arg(arg)\n",
    "    shutil.copy2(inspect.getfile(Model), arg.log_dir)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    '''\n",
    "    Get tf.dataset objects for training and testing data\n",
    "    Data shape: features - batch_size, 3, 300, 25, 2\n",
    "                labels   - batch_size, num_classes\n",
    "    '''\n",
    "    train_data = get_dataset(train_data_path,\n",
    "                             num_classes=num_classes,\n",
    "                             batch_size=global_batch_size,\n",
    "                             drop_remainder=True,\n",
    "                             shuffle=True)\n",
    "    \n",
    "    train_data = strategy.experimental_distribute_dataset(train_data)\n",
    "\n",
    "    test_data = get_dataset(test_data_path,\n",
    "                            num_classes=num_classes,\n",
    "                            batch_size=batch_size,\n",
    "                            drop_remainder=False,\n",
    "                            shuffle=False)\n",
    "\n",
    "    boundaries = [(step*40000)//batch_size for step in steps]\n",
    "    values = [base_lr]*(len(steps)+1)\n",
    "    for i in range(1, len(steps)+1):\n",
    "        values[i] *= 0.1**i\n",
    "    learning_rate  = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model        = Model(num_classes=num_classes)\n",
    "        optimizer    = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                               momentum=0.9,\n",
    "                                               nesterov=True)\n",
    "        ckpt         = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "        ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                                  checkpoint_path,\n",
    "                                                  max_to_keep=5)\n",
    "\n",
    "        # keras metrics to hold accuracies and loss\n",
    "        cross_entropy_loss   = tf.keras.metrics.Mean(name='cross_entropy_loss')\n",
    "        train_acc            = tf.keras.metrics.CategoricalAccuracy(name='train_acc')\n",
    "        train_acc_top_5      = tf.keras.metrics.TopKCategoricalAccuracy(name='train_acc_top_5')\n",
    "\n",
    "    epoch_test_acc       = tf.keras.metrics.CategoricalAccuracy(name='epoch_test_acc')\n",
    "    epoch_test_acc_top_5 = tf.keras.metrics.TopKCategoricalAccuracy(name='epoch_test_acc_top_5')\n",
    "    test_acc_top_5       = tf.keras.metrics.TopKCategoricalAccuracy(name='test_acc_top_5')\n",
    "    test_acc             = tf.keras.metrics.CategoricalAccuracy(name='test_acc')\n",
    "    summary_writer       = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    # Get 1 batch from train dataset to get graph trace of train and test functions\n",
    "    \n",
    "    for data in test_data:\n",
    "        features, labels, index = data\n",
    "        break\n",
    "\n",
    "    # add graph of train and test functions to tensorboard graphs\n",
    "    # Note:\n",
    "    # graph training is True on purpose, allows tensorflow to get all the\n",
    "    # variables, which is required for the first call of @tf.function function\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    train_step(features, labels)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"training_trace\",step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    test_step(features)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"testing_trace\", step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    # start training\n",
    "    train_iter = 0\n",
    "    test_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch+1))\n",
    "        print(\"Training: \")\n",
    "        with strategy.scope():\n",
    "            for features, labels, index in tqdm(train_data):\n",
    "                train_step(features, labels)\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\"cross_entropy_loss\",\n",
    "                                      cross_entropy_loss.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc\",\n",
    "                                      train_acc.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc_top_5\",\n",
    "                                      train_acc_top_5.result(),\n",
    "                                      step=train_iter)\n",
    "                cross_entropy_loss.reset_states()\n",
    "                train_acc.reset_states()\n",
    "                train_acc_top_5.reset_states()\n",
    "                train_iter += 1\n",
    "\n",
    "        print(\"Testing: \")\n",
    "        for features, labels, index in tqdm(test_data):\n",
    "            y_pred = test_step(features)\n",
    "            print(\"test acc \", test_acc(labels, y_pred))\n",
    "            print(\"epoch test acc \", epoch_test_acc(labels, y_pred))\n",
    "            print(\"test acc top5\", test_acc_top_5(labels, y_pred))\n",
    "            print(\"epoch test acc top5 \",epoch_test_acc_top_5(labels, y_pred))\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"test_acc\",\n",
    "                                  test_acc.result(),\n",
    "                                  step=test_iter)\n",
    "                tf.summary.scalar(\"test_acc_top_5\",\n",
    "                                  test_acc_top_5.result(),\n",
    "                                  step=test_iter)\n",
    "            test_acc.reset_states()\n",
    "            test_acc_top_5.reset_states()\n",
    "            test_iter += 1\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"epoch_test_acc\",\n",
    "                              epoch_test_acc.result(),\n",
    "                              step=epoch)\n",
    "            tf.summary.scalar(\"epoch_test_acc_top_5\",\n",
    "                              epoch_test_acc_top_5.result(),\n",
    "                              step=epoch)\n",
    "        epoch_test_acc.reset_states()\n",
    "        epoch_test_acc_top_5.reset_states()\n",
    "\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                ckpt_save_path))\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print('Saving final checkpoint for epoch {} at {}'.format(epochs,\n",
    "\n",
    "                                                              ckpt_save_path))\n",
    "    \n",
    "    # submission_test_data = get_dataset(\"tfrecord_test\",\n",
    "    #                         num_classes=49,\n",
    "    #                         batch_size=64,\n",
    "    #                         drop_remainder=False,\n",
    "    #                         shuffle=False)\n",
    "    \n",
    "    # result=[]\n",
    "    # index_list=[]\n",
    "    # for features, labels, index in tqdm(submission_test_data):\n",
    "    #     pred_index = tf.keras.backend.eval(tf.math.argmax(index, 1))\n",
    "    #     y_pred = test_step(features)\n",
    "    #     pred_label = tf.keras.backend.eval(tf.math.argmax(y_pred, 1))\n",
    "    #     for item in pred_label:\n",
    "    #         result.append(item)\n",
    "    #     for item in pred_index:\n",
    "    #         index_list.append(item)\n",
    "\n",
    "    \n",
    "    # print(\"----------------------------------------------\")\n",
    "    # Id = [i for i in range (0, 2959)]\n",
    "    # predict = pd.DataFrame(Id, columns = ['Id'])\n",
    "    # result_order = [None]  * 2959\n",
    "    # for i, item in enumerate(index_list):\n",
    "    #     result_order[item] = result[i]\n",
    "    # print(result_order)\n",
    "    # predict['Category'] = result_order\n",
    "    # print(predict)\n",
    "    # predict.to_csv('output.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    <ipython-input-12-12628ac89ce4>:16 train_step  *\n        labels_dict = dict(Counter(labels))\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/collections/__init__.py:535 __init__  **\n        self.update(*args, **kwds)\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/collections/__init__.py:622 update\n        _count_elements(self, iterable)\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:561 __iter__\n        self._disallow_iteration()\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:554 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled\n        \" decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-08e5577164bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# variables, which is required for the first call of @tf.function function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training_trace\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    <ipython-input-12-12628ac89ce4>:16 train_step  *\n        labels_dict = dict(Counter(labels))\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/collections/__init__.py:535 __init__  **\n        self.update(*args, **kwds)\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/collections/__init__.py:622 update\n        _count_elements(self, iterable)\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:561 __iter__\n        self._disallow_iteration()\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:554 _disallow_iteration\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n    /home/mlp/anaconda3/envs/SML/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled\n        \" decorating it directly with @tf.function.\".format(task))\n\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission_test_data = get_dataset(\"tfrecord_test\",\n",
    "                        num_classes=49,\n",
    "                        batch_size=64,\n",
    "                        drop_remainder=False,\n",
    "                        shuffle=False)\n",
    "\n",
    "result=[]\n",
    "index_list=[]\n",
    "for features, labels, index in tqdm(submission_test_data):\n",
    "    pred_index = tf.keras.backend.eval(tf.math.argmax(index, 1))\n",
    "    y_pred = test_step(features)\n",
    "    pred_label = tf.keras.backend.eval(tf.math.argmax(y_pred, 1))\n",
    "    for item in pred_label:\n",
    "        result.append(item)\n",
    "    for item in pred_index:\n",
    "        index_list.append(item)\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "Id = [i for i in range (0, 2959)]\n",
    "predict = pd.DataFrame(Id, columns = ['Id'])\n",
    "result_order = [None]  * 2959\n",
    "for i, item in enumerate(index_list):\n",
    "    result_order[item] = result[i]\n",
    "print(result_order)\n",
    "predict['Category'] = result_order\n",
    "print(predict)\n",
    "predict.to_csv('output.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('SML': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "2aad38b272c115067aca7c2763626a41e209a4efedee411e5fae63cf6f4797c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}