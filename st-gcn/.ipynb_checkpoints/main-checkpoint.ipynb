{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daabaeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from STGCN.ipynb\n",
      "importing Jupyter notebook from skeleton.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from STGCN import Model\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import inspect\n",
    "import shutil\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2268ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Spatial Temporal Graph Convolutional Neural Network for Skeleton-Based Action Recognition')\n",
    "    parser.add_argument(\n",
    "        '--base-lr', type=float, default=1e-1, help='initial learning rate')\n",
    "    parser.add_argument(\n",
    "        '--num-classes', type=int, default=49, help='number of classes in dataset')\n",
    "    parser.add_argument(\n",
    "        '--batch-size', type=int, default=64, help='training batch size')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs', type=int, default=50, help='total epochs to train')\n",
    "    parser.add_argument(\n",
    "        '--save-freq', type=int, default=10, help='periodicity of saving model weights')\n",
    "    parser.add_argument(\n",
    "        '--checkpoint-path',\n",
    "        default=\"checkpoints\",\n",
    "        help='folder to store model weights')\n",
    "    parser.add_argument(\n",
    "        '--log-dir',\n",
    "        default=\"logs\",\n",
    "        help='folder to store model-definition/training-logs/hyperparameters')\n",
    "    parser.add_argument(\n",
    "        '--train-data-path',\n",
    "        default=\"tfrecord1\",\n",
    "        help='path to folder with training dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--test-data-path',\n",
    "        default=\"tfrecord1\",\n",
    "        help='path to folder with testing dataset tfrecord files')\n",
    "    parser.add_argument(\n",
    "        '--steps',\n",
    "        type=int,\n",
    "        default=[10, 40],\n",
    "        nargs='+',\n",
    "        help='the epoch where optimizer reduce the learning rate, eg: 10 50')\n",
    "    parser.add_argument(\n",
    "        '--gpus',\n",
    "        default= None,\n",
    "        nargs='+',\n",
    "        help='list of gpus to use for training, eg: \"/gpu:0\" \"/gpu:1\"')\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a6ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory, num_classes=49, batch_size=32, drop_remainder=False,\n",
    "                shuffle=False, shuffle_size=1000):\n",
    "    # dictionary describing the features.\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label'     : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "\n",
    "    # parse each proto and, the features within\n",
    "    def _parse_feature_function(example_proto):\n",
    "        features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "        data =  tf.io.parse_tensor(features['features'], tf.float32)\n",
    "        index = tf.one_hot(features['label'], 2959)\n",
    "        label = tf.one_hot(features['label'], num_classes)\n",
    "        data = tf.reshape(data, (3, 16, 20, 1))\n",
    "        return data, label, index\n",
    "\n",
    "    records = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\"tfrecord\")]\n",
    "    dataset = tf.data.TFRecordDataset(records, num_parallel_reads=len(records))\n",
    "    dataset = dataset.map(_parse_feature_function)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862c0830",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    def step_fn(features, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(features, training=True)\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                              labels=labels)\n",
    "            loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        train_acc(labels, logits)\n",
    "        train_acc_top_5(labels, logits)\n",
    "        cross_entropy_loss(loss)\n",
    "    strategy.run(step_fn, args=(features, labels,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44717555",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(features):\n",
    "    logits = model(features, training=False)\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9450d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Num GPUs Available:  1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14172/3968427280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# variables, which is required for the first call of @tf.function function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"training_trace\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = get_parser()\n",
    "    arg, unknown = parser.parse_known_args()\n",
    "\n",
    "    base_lr         = arg.base_lr\n",
    "    num_classes     = arg.num_classes\n",
    "    epochs          = arg.num_epochs\n",
    "    checkpoint_path = arg.checkpoint_path\n",
    "    log_dir         = arg.log_dir\n",
    "    train_data_path = arg.train_data_path\n",
    "    test_data_path  = arg.test_data_path\n",
    "    save_freq       = arg.save_freq\n",
    "    steps           = arg.steps\n",
    "    batch_size      = arg.batch_size\n",
    "    gpus            = arg.gpus\n",
    "    strategy        = tf.distribute.MirroredStrategy(arg.gpus)\n",
    "    global_batch_size = arg.batch_size*strategy.num_replicas_in_sync\n",
    "    arg.gpus        = strategy.num_replicas_in_sync\n",
    "\n",
    "    #copy hyperparameters and model definition to log folder\n",
    "    #save_arg(arg)\n",
    "    shutil.copy2(inspect.getfile(Model), arg.log_dir)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "    '''\n",
    "    Get tf.dataset objects for training and testing data\n",
    "    Data shape: features - batch_size, 3, 300, 25, 2\n",
    "                labels   - batch_size, num_classes\n",
    "    '''\n",
    "    train_data = get_dataset(train_data_path,\n",
    "                             num_classes=num_classes,\n",
    "                             batch_size=global_batch_size,\n",
    "                             drop_remainder=True,\n",
    "                             shuffle=True)\n",
    "    \n",
    "    train_data = strategy.experimental_distribute_dataset(train_data)\n",
    "\n",
    "    test_data = get_dataset(test_data_path,\n",
    "                            num_classes=num_classes,\n",
    "                            batch_size=batch_size,\n",
    "                            drop_remainder=False,\n",
    "                            shuffle=False)\n",
    "\n",
    "    boundaries = [(step*40000)//batch_size for step in steps]\n",
    "    values = [base_lr]*(len(steps)+1)\n",
    "    for i in range(1, len(steps)+1):\n",
    "        values[i] *= 0.1**i\n",
    "    learning_rate  = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "    with strategy.scope():\n",
    "        model        = Model(num_classes=num_classes)\n",
    "        optimizer    = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                               momentum=0.9,\n",
    "                                               nesterov=True)\n",
    "        ckpt         = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "        ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                                  checkpoint_path,\n",
    "                                                  max_to_keep=5)\n",
    "\n",
    "        # keras metrics to hold accuracies and loss\n",
    "        cross_entropy_loss   = tf.keras.metrics.Mean(name='cross_entropy_loss')\n",
    "        train_acc            = tf.keras.metrics.CategoricalAccuracy(name='train_acc')\n",
    "        train_acc_top_5      = tf.keras.metrics.TopKCategoricalAccuracy(name='train_acc_top_5')\n",
    "\n",
    "    epoch_test_acc       = tf.keras.metrics.CategoricalAccuracy(name='epoch_test_acc')\n",
    "    epoch_test_acc_top_5 = tf.keras.metrics.TopKCategoricalAccuracy(name='epoch_test_acc_top_5')\n",
    "    test_acc_top_5       = tf.keras.metrics.TopKCategoricalAccuracy(name='test_acc_top_5')\n",
    "    test_acc             = tf.keras.metrics.CategoricalAccuracy(name='test_acc')\n",
    "    summary_writer       = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    # Get 1 batch from train dataset to get graph trace of train and test functions\n",
    "    \n",
    "    for data in test_data:\n",
    "        features, labels, index = data\n",
    "        break\n",
    "\n",
    "    # add graph of train and test functions to tensorboard graphs\n",
    "    # Note:\n",
    "    # graph training is True on purpose, allows tensorflow to get all the\n",
    "    # variables, which is required for the first call of @tf.function function\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    train_step(features, labels)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"training_trace\",step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    tf.summary.trace_on(graph=True)\n",
    "    test_step(features)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"testing_trace\", step=0)\n",
    "    tf.summary.trace_off()\n",
    "\n",
    "    # start training\n",
    "    train_iter = 0\n",
    "    test_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch+1))\n",
    "        print(\"Training: \")\n",
    "        with strategy.scope():\n",
    "            for features, labels, index in tqdm(train_data):\n",
    "                train_step(features, labels)\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\"cross_entropy_loss\",\n",
    "                                      cross_entropy_loss.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc\",\n",
    "                                      train_acc.result(),\n",
    "                                      step=train_iter)\n",
    "                    tf.summary.scalar(\"train_acc_top_5\",\n",
    "                                      train_acc_top_5.result(),\n",
    "                                      step=train_iter)\n",
    "                cross_entropy_loss.reset_states()\n",
    "                train_acc.reset_states()\n",
    "                train_acc_top_5.reset_states()\n",
    "                train_iter += 1\n",
    "\n",
    "        print(\"Testing: \")\n",
    "        for features, labels, index in tqdm(test_data):\n",
    "            y_pred = test_step(features)\n",
    "            print(\"test acc \", test_acc(labels, y_pred))\n",
    "            print(\"epoch test acc \", epoch_test_acc(labels, y_pred))\n",
    "            print(\"test acc top5\", test_acc_top_5(labels, y_pred))\n",
    "            print(\"epoch test acc top5 \",epoch_test_acc_top_5(labels, y_pred))\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar(\"test_acc\",\n",
    "                                  test_acc.result(),\n",
    "                                  step=test_iter)\n",
    "                tf.summary.scalar(\"test_acc_top_5\",\n",
    "                                  test_acc_top_5.result(),\n",
    "                                  step=test_iter)\n",
    "            test_acc.reset_states()\n",
    "            test_acc_top_5.reset_states()\n",
    "            test_iter += 1\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"epoch_test_acc\",\n",
    "                              epoch_test_acc.result(),\n",
    "                              step=epoch)\n",
    "            tf.summary.scalar(\"epoch_test_acc_top_5\",\n",
    "                              epoch_test_acc_top_5.result(),\n",
    "                              step=epoch)\n",
    "        epoch_test_acc.reset_states()\n",
    "        epoch_test_acc_top_5.reset_states()\n",
    "\n",
    "        if (epoch + 1) % save_freq == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                ckpt_save_path))\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print('Saving final checkpoint for epoch {} at {}'.format(epochs,\n",
    "\n",
    "                                                              ckpt_save_path))\n",
    "    \n",
    "    test_data = get_dataset(\"tfrecord_test1\",\n",
    "                            num_classes=49,\n",
    "                            batch_size=64,\n",
    "                            drop_remainder=False,\n",
    "                            shuffle=False)\n",
    "    result=[]\n",
    "    index_list=[]\n",
    "    for features, labels, index in tqdm(test_data):\n",
    "        pred_index = tf.keras.backend.eval(tf.math.argmax(index, 1))\n",
    "        y_pred = test_step(features)\n",
    "        pred_label = tf.keras.backend.eval(tf.math.argmax(y_pred, 1))\n",
    "        for item in pred_label:\n",
    "            result.append(item)\n",
    "        for item in pred_index:\n",
    "            index_list.append(item)\n",
    "\n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    Id = [i for i in range (0, 2959)]\n",
    "    predict = pd.DataFrame(Id, columns = ['Id'])\n",
    "    result_order = [None]  * 2959\n",
    "    for i, item in enumerate(index_list):\n",
    "        result_order[item] = result[i]\n",
    "    print(result_order)\n",
    "    predict['Category'] = result_order\n",
    "    print(predict)\n",
    "    predict.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
