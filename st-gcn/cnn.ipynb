{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "import import_ipynb\n",
                "from rotation import rotation_matrix, unit_vector, angle_between, x_rotation, y_rotation, z_rotation\n",
                "\n",
                "# for machine learning\n",
                "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
                "from sklearn.model_selection import train_test_split\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.combine import SMOTETomek, SMOTEENN\n",
                "from imblearn.under_sampling import TomekLinks, RandomUnderSampler, EditedNearestNeighbours, AllKNN\n",
                "\n",
                "import csv\n",
                "from csv import reader\n",
                "import pickle\n",
                "from collections import Counter\n",
                "from matplotlib import pyplot"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "importing Jupyter notebook from rotation.ipynb\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "NUM_FEATURES = 3\n",
                "NUM_JOINTS = 20\n",
                "NUM_FRAMES = 16\n",
                "FILE_NAME = '../train.csv'\n",
                "test_FILE_NAME = '../test.csv'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "dtf = pd.read_csv(FILE_NAME, header = None)\n",
                "\n",
                "X = dtf.iloc[:,:-1]\n",
                "y = dtf.iloc[:,-1]\n",
                "\n",
                "dtf_test = pd.read_csv(test_FILE_NAME, header = None)\n",
                "#dtf_test = dtf_test.set_index(dtf_test.columns[0])\n",
                "\n",
                "X_test = dtf_test.iloc[:,:]\n",
                "X_test_index = dtf_test.iloc[:,0]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def read_xyz(row):\n",
                "    skeleton_data, label = read_skeleton(row)\n",
                "    \n",
                "    data = np.zeros((NUM_FRAMES, NUM_JOINTS, NUM_FEATURES))\n",
                "    for m, i in enumerate(skeleton_data['frame_info']):\n",
                "        for n, j in enumerate(i['joint_info']):\n",
                "            feature_info = j['feature_info']\n",
                "            data[m, n, :] = [feature_info['x'], feature_info['y'], feature_info['z']]\n",
                "\n",
                "    # data = data.transpose(2, 0, 1)\n",
                "    return data, label"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "def read_skeleton(row):\n",
                "    label = row[-1]\n",
                "    #label = -1\n",
                "    skeleton_data = {}\n",
                "    skeleton_data['index'] = row[0]\n",
                "    skeleton_data['num_frame'] = NUM_FRAMES\n",
                "    skeleton_data['frame_info'] = []\n",
                "    \n",
                "    for frame in range(NUM_FRAMES):\n",
                "        offset = NUM_JOINTS * NUM_FEATURES\n",
                "        data_in_frame = row[frame*offset:(frame+1)*offset]\n",
                "        frame_info = {}\n",
                "        frame_info['num_joints'] = NUM_JOINTS\n",
                "        frame_info['joint_info'] = []\n",
                "        \n",
                "        for feature in range(NUM_JOINTS):\n",
                "            offset = NUM_FEATURES\n",
                "            data_in_joint = data_in_frame[feature*offset:(feature+1)*offset]\n",
                "            joint_info = {}\n",
                "            joint_info['num_features'] = NUM_FEATURES\n",
                "            joint_info['feature_info'] = {\n",
                "                k: float(v)\n",
                "                for k, v in zip(['x', 'y', 'z'], data_in_joint)\n",
                "            }\n",
                "            frame_info['joint_info'].append(joint_info)\n",
                "                    \n",
                "        skeleton_data['frame_info'].append(frame_info)\n",
                "    return skeleton_data, label"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "# create train and test datasets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
                "                                                    test_size=0.3, \n",
                "                                                    stratify=np.array(y), \n",
                "                                                    random_state=42)\n",
                "\n",
                "# create train and validation datasets\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
                "                                                  test_size=0.15, \n",
                "                                                  stratify=np.array(y_train), \n",
                "                                                  random_state=42)\n",
                "\n",
                "\n",
                "print('Initial Dataset Size:', X.shape)\n",
                "print('Initial Train and Test Datasets Size:', X_train.shape, X_test.shape)\n",
                "print('Train and Validation Datasets Size:', X_train.shape, X_val.shape)\n",
                "print('Train, Test and Validation Datasets Size:', X_train.shape, X_test.shape, X_val.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Initial Dataset Size: (9388, 961)\n",
                        "Initial Train and Test Datasets Size: (5585, 961) (2817, 961)\n",
                        "Train and Validation Datasets Size: (5585, 961) (986, 961)\n",
                        "Train, Test and Validation Datasets Size: (5585, 961) (2817, 961) (986, 961)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "fp = np.zeros((len(X_train), NUM_FRAMES, NUM_JOINTS, NUM_FEATURES), dtype=np.float32) #construct a matrix, with num of data, num of features for each joint, num of frames, num of joints, num of people(always 1 in our case)\n",
                "print(len(X_train))\n",
                "for i, row in enumerate(X_train.to_numpy()):\n",
                "    data, label = read_xyz(row)\n",
                "    fp[i, :, :, :] = data\n",
                "#fp = normalisation(fp)\n",
                "# np.save('train_smote_custom.npy', fp)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "7510\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "fp.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(7510, 16, 20, 3)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "# Create train generator.\n",
                "train_datagen = ImageDataGenerator(rescale=1./255, \n",
                "                                   rotation_range=30, \n",
                "                                   width_shift_range=0.2,\n",
                "                                   height_shift_range=0.2, \n",
                "                                   horizontal_flip = 'true')\n",
                "train_generator = train_datagen.flow(x_train, y_train_ohe, shuffle=False, \n",
                "                                     batch_size=BATCH_SIZE, seed=1)\n",
                "                                     \n",
                "# Create validation generator\n",
                "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
                "val_generator = train_datagen.flow(x_val, y_val_ohe, shuffle=False, \n",
                "                                   batch_size=BATCH_SIZE, seed=1)       "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from keras.models import Model\n",
                "from keras.optimizers import Adam\n",
                "from keras.layers import GlobalAveragePooling2D\n",
                "from keras.layers import Dense\n",
                "from keras.applications.inception_v3 import InceptionV3\n",
                "from keras.utils.np_utils import to_categorical\n",
                "\n",
                "# Get the InceptionV3 model so we can do transfer learning\n",
                "base_inception = InceptionV3(weights='imagenet', include_top=False, \n",
                "                             input_shape=(299, 299, 3))\n",
                "                             \n",
                "# Add a global spatial average pooling layer\n",
                "out = base_inception.output\n",
                "out = GlobalAveragePooling2D()(out)\n",
                "out = Dense(512, activation='relu')(out)\n",
                "out = Dense(512, activation='relu')(out)\n",
                "total_classes = y_train_ohe.shape[1]\n",
                "predictions = Dense(total_classes, activation='softmax')(out)\n",
                "\n",
                "model = Model(inputs=base_inception.input, outputs=predictions)\n",
                "\n",
                "# only if we want to freeze layers\n",
                "for layer in base_inception.layers:\n",
                "    layer.trainable = False\n",
                "    \n",
                "# Compile \n",
                "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy']) \n",
                "model.summary()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# labels = []\n",
                "# for i, row in enumerate(y_train.values):\n",
                "#     labels.append(row)\n",
                "# with open('label_cnn.pkl', 'wb') as f:\n",
                "#     pickle.dump(labels, f)"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.13",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.13 64-bit ('SML': conda)"
        },
        "interpreter": {
            "hash": "2aad38b272c115067aca7c2763626a41e209a4efedee411e5fae63cf6f4797c6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}